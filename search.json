[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello!\nI am Daniel Park, an exchange student from UNSW in Australia. I am here for one semester and I am excited to see what UPenn has to offer!\nAn interesting fact about me would be that I was actually born in Los Angeles, but moved to South Korea for a couple of years, before moving again to Australia where I study now. As a result, I have both an American and Australian passport.\nThis blog will outline my exploration of data and the analyses / conclusions which are derived from it."
  },
  {
    "objectID": "posts/blog_01/DOW1_weather.html",
    "href": "posts/blog_01/DOW1_weather.html",
    "title": "Blog 01: Analysis Weather Trends in Philadelphia",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nweather_df = pd.read_csv('data/philadelphia_weather_2005_to_2025.csv')\nweather_df = weather_df.assign(date=pd.to_datetime(weather_df['date']))"
  },
  {
    "objectID": "posts/blog_01/DOW1_weather.html#analysis",
    "href": "posts/blog_01/DOW1_weather.html#analysis",
    "title": "Blog 01: Analysis Weather Trends in Philadelphia",
    "section": "Analysis",
    "text": "Analysis\nChristmas weather in Philadelphia is notoriously unpredictable. To understand how unusual this variability really is, this blog will examine daily weather records from 2005 - 2025, focusing on Christmas Day and placing it in the context of broader temperature, snowfall, and extreme weather trends.\nWe will ultimately determine if it is worth staying in Philadelphia for the holiday season.\n\nHow variable is the temperature on Christmas day in Philadelphia?\n\nchristmas_df = weather_df.query(\"day=='December 25'\").set_index('year')[['high','low']]\nchristmas_df.plot(kind = 'line', style = '-o')\nplt.title(\"Changes in Low and High Temperature for Christmas Day over Time\")\nplt.ylabel(\"Temperature\")\nplt.xlabel(\"Year\")\n\nplt.show()\n\n\n\n\n\n\n\n\nChristmas Day temperatures fluctuate substantially from year to year, with no clear long term trend over the period.\n\nprint(f\"Hottest Christmas: {christmas_df['high'].idxmax()}, {christmas_df['high'].max()} degrees F.\")\nprint(f\"Coldest Christmas: {christmas_df['low'].idxmin()}, {christmas_df['low'].min()} degrees F.\")\nprint(f\"Greatest temperature change: {(christmas_df['high'] - christmas_df['low']).idxmax()}, with a temperature difference between high and low of {(christmas_df['high'] - christmas_df['low']).max()} degrees Farenheit.\")\n\nHottest Christmas: 2015, 68 degrees F.\nColdest Christmas: 2022, 18 degrees F.\nGreatest temperature change: 2020, with a temperature difference between high and low of 39 degrees Farenheit.\n\n\nThe hottest Christmas in the dataset was in 2015, with a high temperature of 68 degrees Farenheit. An additional interesting fact is that 2015 was the hottest Christmas in any year from 1874 to 2025, tied only with Christmas in 1964. (According to https://www.stormfax.com/phlChristmasWx.htm)\nThe coldest Christmas in the dataset was in 2022, with a low temperature of 18 degrees Farenheit. However, this is still 17 degrees Farenheit warmer than the coldest Christmas day, which was in 1980 and 1983, which had a minimum temperature of just 1 degree Farenheit. (https://www.stormfax.com/phlChristmasWx.htm)\nThe Christmas with the greatest temperature change was in 2020, with a temperature difference between high and low of 39 degrees Farenheit.\n\n\nChanges in Annual Snowfall\n\nannual_snow = weather_df.groupby('year')['snow'].sum()\n\nx = annual_snow.index.values\ny = annual_snow.values\n\nslope, intercept = np.polyfit(x, y, 1)\ny_hat = intercept + slope * x\n\nplt.figure(figsize=(8,5))\n\nplt.plot(x, y, '-o', label='Observed')\nplt.plot(x, y_hat, '--', label='Regression line')\n\nplt.title(\"Yearly Total Snowfall, 2005–2025\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Snowfall (inches)\")\n\nplt.xticks(annual_snow.index[::2])\nplt.legend()\nplt.show()\n\nannual_snow.sort_values(ascending = True).head(5)\nannual_snow.sort_values(ascending = False).head(5)\n\n\n\n\n\n\n\n\nyear\n2010    67.3\n2014    56.9\n2009    45.6\n2005    37.0\n2011    31.6\nName: snow, dtype: float64\n\n\nThe fitted regression line suggests a decline in total snowfall over time, although snowfall remains highly variable from year to year.\n2010 saw the greatest amount of snow in the data set receiving 67.3 inches, which is 11.6 inches more than 2014, the year with the second highest snowfall. An interesting fact is that during this year’s snowstorm on January 26, certain parts of Philadelphia received 9.3 inches of snow, which is already more snow than the total yearly snowfall observed in 2023, 2012, 2008 and 2020.\n\n\nLong Term Temperature Trends\n\nweather_df['avg'] = (weather_df['high'] + weather_df['low']) / 2\n\nyearly_avg = weather_df.groupby('year')['avg'].mean()\n\n# Obtaining the index and values for the plot.\nx = yearly_avg.index.values\ny = yearly_avg.values\n\n# Creating a regression line.\nslope, intercept = np.polyfit(x, y, 1)\ny_hat = intercept + slope * x\n\n# Increasing size of plot.\nplt.figure(figsize=(10,6))\n\n# Plotting observed data and the regression line on the same plot.\nplt.plot(x, y, '-o', label='Observed')\nplt.plot(x, y_hat, '--', label='Regression line')\n\nplt.title(\"Yearly Average Temperature, 2005–2025\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature\")\n\n# Adjusting the indexing so only every two years labelled.\nplt.xticks(annual_snow.index[::2])\nplt.legend()\nplt.show()\n\nprint(f\"Equation: y = {slope:.4f} * x + {intercept:.4f}\")\n\n\n\n\n\n\n\n\nEquation: y = 0.0907 * x + -125.2568\n\n\nWe can see that there were many fluctuations in the yearly average temperature, with peaks in 2006, 2012, 2016, 2021, 2024.\nThe plotted regression line on the graph shows that the yearly average temperature increases as the years went by. More specifically, every year, the yearly average temperature goes up by 0.0907 degrees Farenheit on average. The fitted trend suggests an increase in average temperature over this period, though year-to-year variability is still large.\n\n\nChanges in Average Temperature in December over the Years\n\ndecember_df = weather_df[weather_df['date'].dt.month == 12]\n\ndec_avg_by_year = december_df.groupby(december_df['date'].dt.year)['avg'].mean()\n\n# Regression line\nx = dec_avg_by_year.index.values.astype(float)\ny = dec_avg_by_year.values.astype(float)\n\nslope, intercept = np.polyfit(x, y, 1)\ny_hat = intercept + slope * x\n\nprint(f\"Equation: y = {slope:.4f} * x + {intercept:.4f}\")\n\n# Plot\nplt.figure(figsize=(10,6))\nplt.plot(x, y, '-o', label='Observed (Dec average)')\nplt.plot(x, y_hat, '--', label='Linear trend')\n\nplt.title(\"Average Temperature in December by Year\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Average Temperature (°F)\")\nplt.xticks(x[::2])\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nEquation: y = 0.0760 * x + -113.4909\n\n\n\n\n\n\n\n\n\nThe regression line suggests that the average December temperature has been rising slowly throughout time, increasing an average of 0.076 degrees Farenheit every year. We can also see that there was an unusually warm December in 2015, with an average temperature above 50 degrees Farenheit.\n\n\nTemperature Extremes over Time\n\nhighest_temps = weather_df.groupby('year')['high'].max()\nlowest_temps = weather_df.groupby('year')['low'].min()\n\nx = highest_temps.index.values\n\nplt.figure(figsize=(10,6))\n\nplt.plot(x, highest_temps, label='Annual max temp')\nplt.plot(x, lowest_temps, label='Annual min temp')\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature\")\nplt.title(\"Annual Highest and Lowest Temperatures\")\nplt.legend()\nplt.show()\n\nmean_max_temp = highest_temps.mean()\nmean_min_temp = lowest_temps.mean()\n\nprint(f\"Mean annual max temp: {mean_max_temp:.2f}\")\nprint(f\"Mean annual min temp: {mean_min_temp:.2f}\")\n\nstd_max_temp = highest_temps.std()\nstd_min_temp = lowest_temps.std()\n\nprint(f\"Std dev of annual max temp: {std_max_temp:.2f}\")\nprint(f\"Std dev of annual min temp: {std_min_temp:.2f}\")\n\nyear_highest_max = highest_temps.idxmax()\nyear_lowest_min = lowest_temps.idxmin()\n\nprint(f\"Year with highest annual max temp: {year_highest_max}, temp: {highest_temps.max()}\")\nprint(f\"Year with lowest annual min temp: {year_lowest_min}, temp: {lowest_temps.min()}\")\n\n\n\n\n\n\n\n\nMean annual max temp: 98.19\nMean annual min temp: 9.52\nStd dev of annual max temp: 2.16\nStd dev of annual min temp: 4.57\nYear with highest annual max temp: 2010, temp: 103\nYear with lowest annual min temp: 2015, temp: 2\n\n\nThe annual maximum temperatures were fairly consistent throughout the 20 years, with a standard deviation of 2.16 degrees and an average maximum temperature of 98.19 degrees. The year with the hottest day was in 2010, with a temperature of 103 degrees, which interestingly enough was the year with the highest recorded annual snowfall in the dataset.\nAnnual minimum temperatures have greater temperature fluctuation, with standard deviation of 4.57 degrees. The average annual minimum temperatures was 9.52 degrees. The year with the coldest day was in 2015 at 2 degrees, but this is interesting as it had an unusually warm December as outlined previously."
  },
  {
    "objectID": "posts/blog_01/DOW1_weather.html#conclusion",
    "href": "posts/blog_01/DOW1_weather.html#conclusion",
    "title": "Blog 01: Analysis Weather Trends in Philadelphia",
    "section": "Conclusion",
    "text": "Conclusion\nWeather patterns in Philadelphia over the past two decades point to a combination of gradual warming, declining snowfall, and substantial variability in winter conditions. Christmas Day, in particular, shows no consistent pattern. However, analysis of the weather more broadly in the past 20 years indicates a gradual increase in average temperatures alongside fewer sustained periods of heavy snowfall.\nOverall, these findings suggest that while Philadelphia winters remain highly variable, recent decades have been characterised by warmer average temperatures and reduced snowfall. Although individual years may deviate from this pattern, the long-term trends indicate that snow-filled December holiday seasons are less typical than they were two decades ago."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel’s Data Stories",
    "section": "",
    "text": "Blog 02: Analysis of Personality Trends\n\n\n\npersonality\n\ndata stories\n\ndata visualization\n\n\n\nUsing the BIG5 personality test, we will answer the question: What actually makes a person extraverted?\n\n\n\n\n\nFeb 11, 2026\n\n\nDaniel Park\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 01: Analysis Weather Trends in Philadelphia\n\n\n\nweather\n\ndata stories\n\ndata visualization\n\n\n\nUsing two decades of daily weather data from Philadelphia, this post explores how temperatures, snowfall, and weather extremes have changed over time, revealing what the Christmas season in Philly has really been like over the past 20 years.\n\n\n\n\n\nFeb 6, 2026\n\n\nDaniel Park\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog_02_personality/DOW2_personality.html",
    "href": "posts/blog_02_personality/DOW2_personality.html",
    "title": "Blog 02: Analysis of Personality Trends",
    "section": "",
    "text": "Exploring the BIG5 dataset from the Open-Source Psychometrics Project\n\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\n\nbig5_df = pd.read_csv('data/openpsych_data.csv', sep='\\t')\n\n\nrace\nChosen from a drop down menu.\n1=Mixed Race, 2=Arctic (Siberian, Eskimo), 3=Caucasian (European), 4=Caucasian (Indian), 5=Caucasian (Middle East), 6=Caucasian (North African, Other), 7=Indigenous Australian, 8=Native American, 9=North East Asian (Mongol, Tibetan, Korean Japanese, etc), 10=Pacific (Polynesian, Micronesian, etc), 11=South East Asian (Chinese, Thai, Malay, Filipino, etc), 12=West African, Bushmen, Ethiopian, 13=Other (0=missed)\n\nrace_values = '''1=Mixed Race, 2=Arctic (Siberian, Eskimo), 3=Caucasian (European), \n4=Caucasian (Indian), 5=Caucasian (Middle East), 6=Caucasian (North African, Other), \n7=Indigenous Australian, 8=Native American, \n9=North East Asian (Mongol, Tibetan, Korean Japanese, etc), \n10=Pacific (Polynesian, Micronesian, etc), \n11=South East Asian (Chinese, Thai, Malay, Filipino, etc), 12=West African, Bushmen, Ethiopian, 13=Other, 0=missed\n'''\n\nparts = re.split(r'(?:,\\s+)?([0-9]+)=', race_values.strip())\n\nrace_keys = [int(key_val) for key_val in parts[1::2]]\nrace_values = parts[2::2]\n\n\nrace_mapping = dict(zip(race_keys,race_values))\n\n\nbig5_df=big5_df.assign(race_cat=big5_df['race'].map(race_mapping))\n\n\n\nage\nage entered as text (individuals reporting age &lt; 13 were not recorded)\n\nbig5_df['age'].max()\n\nnp.int64(999999999)\n\n\n\nbig5_df['age'].value_counts().tail(50)\n\nage\n68           20\n67           19\n69           15\n70           12\n71           11\n1992          9\n72            8\n1994          8\n1996          7\n1995          5\n1993          5\n75            5\n1989          5\n1997          4\n1982          4\n1998          4\n1991          3\n1990          3\n77            3\n188           2\n1976          2\n1984          2\n74            2\n76            2\n73            2\n1986          2\n79            2\n1985          2\n1999          1\n1988          1\n100           1\n208           1\n999999999     1\n1961          1\n1977          1\n412434        1\n92            1\n2000          1\n80            1\n1974          1\n97            1\n1968          1\n211           1\n223           1\n99            1\n266           1\n191           1\n78            1\n1964          1\n118           1\nName: count, dtype: int64\n\n\n\nbig5_df[big5_df['age']&gt;100]['age'].value_counts()\n\nage\n1992         9\n1994         8\n1996         7\n1995         5\n1993         5\n1989         5\n1997         4\n1982         4\n1998         4\n1991         3\n1990         3\n1984         2\n1985         2\n188          2\n1976         2\n1986         2\n999999999    1\n208          1\n1988         1\n1999         1\n1977         1\n2000         1\n1961         1\n412434       1\n1974         1\n1968         1\n211          1\n223          1\n266          1\n191          1\n1964         1\n118          1\nName: count, dtype: int64\n\n\n\n\ngender\ngender Chosen from a drop down menu. 1=Male, 2=Female, 3=Other (0=missed)\n\ngender_map = {\n    0: pd.NA,\n    1: 'male',\n    2: 'female',\n    3: 'other'\n}\n\n\nbig5_df['gender'].value_counts()\n\ngender\n2    11985\n1     7608\n3      102\n0       24\nName: count, dtype: int64\n\n\n\nbig5_df=big5_df.assign(gender_cat=big5_df['gender'].map(gender_map))\n\n\nbig5_df['gender_cat'].value_counts()\n\ngender_cat\nfemale    11985\nmale       7608\nother       102\nName: count, dtype: int64\n\n\n\n\nhand\nhand “What hand do you use to write with?”. 1=Right, 2=Left, 3=Both (0=missed)\n\nbig5_df['hand'].value_counts()\n\nhand\n1    17424\n2     1724\n3      471\n0      100\nName: count, dtype: int64\n\n\n\n\nsource\nHow the participant came to the test. Based on HTTP Referer. 1=from another page on the test website, 2=from google, 3=from facebook, 4=from any url with “.edu” in its domain name (e.g. xxx.edu, xxx.edu.au), 6=other source, or HTTP Referer not provided.\n\nbig5_df['source'].value_counts()\n\nsource\n1    12099\n2     3653\n5     3527\n3      303\n4      137\nName: count, dtype: int64\n\n\n\n\ncountry\nThe participant’s technical location. ISO country code.\n\nbig5_df['country'].value_counts().head(20)\n\ncountry\nUS     8753\nGB     1531\nIN     1464\nAU      974\nCA      924\nPH      649\n(nu     369\nIT      277\nMY      247\nPK      222\nDE      191\nZA      179\nBR      175\nID      172\nSE      169\nNZ      157\nNO      147\nRO      135\nSG      133\nNL      133\nName: count, dtype: int64\n\n\n\nbig5_df['country'].nunique()\n\n158\n\n\n\nquestions = '''\nE1  I am the life of the party.\nE2  I don't talk a lot.\nE3  I feel comfortable around people.\nE4  I keep in the background.\nE5  I start conversations.\nE6  I have little to say.\nE7  I talk to a lot of different people at parties.\nE8  I don't like to draw attention to myself.\nE9  I don't mind being the center of attention.\nE10 I am quiet around strangers.\nN1  I get stressed out easily.\nN2  I am relaxed most of the time.\nN3  I worry about things.\nN4  I seldom feel blue.\nN5  I am easily disturbed.\nN6  I get upset easily.\nN7  I change my mood a lot.\nN8  I have frequent mood swings.\nN9  I get irritated easily.\nN10 I often feel blue.\nA1  I feel little concern for others.\nA2  I am interested in people.\nA3  I insult people.\nA4  I sympathize with others' feelings.\nA5  I am not interested in other people's problems.\nA6  I have a soft heart.\nA7  I am not really interested in others.\nA8  I take time out for others.\nA9  I feel others' emotions.\nA10 I make people feel at ease.\nC1  I am always prepared.\nC2  I leave my belongings around.\nC3  I pay attention to details.\nC4  I make a mess of things.\nC5  I get chores done right away.\nC6  I often forget to put things back in their proper place.\nC7  I like order.\nC8  I shirk my duties.\nC9  I follow a schedule.\nC10 I am exacting in my work.\nO1  I have a rich vocabulary.\nO2  I have difficulty understanding abstract ideas.\nO3  I have a vivid imagination.\nO4  I am not interested in abstract ideas.\nO5  I have excellent ideas.\nO6  I do not have a good imagination.\nO7  I am quick to understand things.\nO8  I use difficult words.\nO9  I spend time reflecting on things.\nO10 I am full of ideas.\n'''\n\n\nbig5_questions_df = pd.DataFrame([item.split('\\t') for item in questions.splitlines() if item&gt;''])\n\n\n\n\nQuestions and direction key\nhttps://ipip.ori.org/new_ipip-50-item-scale.htm\n\nfactor_map = { 1: 'E', \n               2: 'A',\n               3: 'C',\n               4: 'N',\n               5: 'O' }\n\n\nipip_df = pd.read_html('big5_questions.html', header=0)[0]\nipip_df = ipip_df.rename(columns={'Unnamed: 1': 'text', 'Unnamed: 7': 'factor_and_direction'})[['text','factor_and_direction']]\nipip_df[['factor','direction']]=ipip_df['factor_and_direction'].str.extract(r'([1-5])(.)')\nipip_df['category']=ipip_df['factor'].astype(int).map(factor_map)\n\n\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\nFile /opt/jupyterhub/share/jupyter/venv/python3_comm3180/lib/python3.12/site-packages/pandas/io/html.py:798, in _LxmlFrameParser._build_doc(self)\n    797 try:\n--&gt; 798     r = parse(self.io, parser=parser)\n    799 except OSError as err:\n\nFile /opt/jupyterhub/share/jupyter/venv/python3_comm3180/lib/python3.12/site-packages/lxml/html/__init__.py:918, in parse(filename_or_url, parser, base_url, **kw)\n    917     parser = html_parser\n--&gt; 918 return etree.parse(filename_or_url, parser, base_url=base_url, **kw)\n\nFile src/lxml/etree.pyx:3711, in lxml.etree.parse()\n\nFile src/lxml/parser.pxi:2031, in lxml.etree._parseDocument()\n\nFile src/lxml/parser.pxi:2057, in lxml.etree._parseDocumentFromURL()\n\nFile src/lxml/parser.pxi:1958, in lxml.etree._parseDocFromFile()\n\nFile src/lxml/parser.pxi:1230, in lxml.etree._BaseParser._parseDocFromFile()\n\nFile src/lxml/parser.pxi:647, in lxml.etree._ParserContext._handleParseResultDoc()\n\nFile src/lxml/parser.pxi:765, in lxml.etree._handleParseResult()\n\nFile src/lxml/parser.pxi:687, in lxml.etree._raiseParseError()\n\nOSError: Error reading file 'big5_questions.html': failed to load \"big5_questions.html\": No such file or directory\n\nThe above exception was the direct cause of the following exception:\n\nFileNotFoundError                         Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 ipip_df = pd.read_html('big5_questions.html', header=0)[0]\n      2 ipip_df = ipip_df.rename(columns={'Unnamed: 1': 'text', 'Unnamed: 7': 'factor_and_direction'})[['text','factor_and_direction']]\n      3 ipip_df[['factor','direction']]=ipip_df['factor_and_direction'].str.extract(r'([1-5])(.)')\n\nFile /opt/jupyterhub/share/jupyter/venv/python3_comm3180/lib/python3.12/site-packages/pandas/io/html.py:1227, in read_html(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\n   1223 check_dtype_backend(dtype_backend)\n   1225 io = stringify_path(io)\n-&gt; 1227 return _parse(\n   1228     flavor=flavor,\n   1229     io=io,\n   1230     match=match,\n   1231     header=header,\n   1232     index_col=index_col,\n   1233     skiprows=skiprows,\n   1234     parse_dates=parse_dates,\n   1235     thousands=thousands,\n   1236     attrs=attrs,\n   1237     encoding=encoding,\n   1238     decimal=decimal,\n   1239     converters=converters,\n   1240     na_values=na_values,\n   1241     keep_default_na=keep_default_na,\n   1242     displayed_only=displayed_only,\n   1243     extract_links=extract_links,\n   1244     dtype_backend=dtype_backend,\n   1245     storage_options=storage_options,\n   1246 )\n\nFile /opt/jupyterhub/share/jupyter/venv/python3_comm3180/lib/python3.12/site-packages/pandas/io/html.py:983, in _parse(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\n    972 p = parser(\n    973     io,\n    974     compiled_match,\n   (...)    979     storage_options,\n    980 )\n    982 try:\n--&gt; 983     tables = p.parse_tables()\n    984 except ValueError as caught:\n    985     # if `io` is an io-like object, check if it's seekable\n    986     # and try to rewind it before trying the next parser\n    987     if hasattr(io, \"seekable\") and io.seekable():\n\nFile /opt/jupyterhub/share/jupyter/venv/python3_comm3180/lib/python3.12/site-packages/pandas/io/html.py:241, in _HtmlFrameParser.parse_tables(self)\n    233 def parse_tables(self):\n    234     \"\"\"\n    235     Parse and return all tables from the DOM.\n    236 \n   (...)    239     list of parsed (header, body, footer) tuples from tables.\n    240     \"\"\"\n--&gt; 241     tables = self._parse_tables(self._build_doc(), self.match, self.attrs)\n    242     return (self._parse_thead_tbody_tfoot(table) for table in tables)\n\nFile /opt/jupyterhub/share/jupyter/venv/python3_comm3180/lib/python3.12/site-packages/pandas/io/html.py:800, in _LxmlFrameParser._build_doc(self)\n    798         r = parse(self.io, parser=parser)\n    799     except OSError as err:\n--&gt; 800         raise FileNotFoundError(\n    801             f\"[Errno {errno.ENOENT}] {os.strerror(errno.ENOENT)}: {self.io}\"\n    802         ) from err\n    803 try:\n    804     r = r.getroot()\n\nFileNotFoundError: [Errno 2] No such file or directory: big5_questions.html\n\n\n\n\nipip_df = ipip_df.assign(number=np.repeat(np.arange(1,11),5))\nipip_df = ipip_df.assign(qcode=ipip_df['category'].str.cat(ipip_df['number'].astype(str)))   \n\n\nneg_items = ipip_df.query('direction==\"-\"')['qcode']\n\n\nMake a copy of the original dataframe to keep available in case of mistakes\n\n\nbig5_scored_df = big5_df.copy()\n\n\nReverse code the negatively keyed items\n\n\nbig5_scored_df[neg_items] = 6-big5_df[neg_items]\n\n\nE_cols = [f'E{n+1}' for n in range(10)]\nE_cols\n\n\ncat_cols = {\n    cat : [f'{cat}{n+1}' for n in range(10)] \n    for cat in ('O','C','E','A','N') \n}\n\n\nfor cat, cols in cat_cols.items():\n    big5_scored_df[cat]=big5_scored_df[cols].sum(axis=1)\n\n\n\n\nWorking\n\nbig5_scored_df\n\n\nCorrelation between extraversion and agreeableness?\nDoes a person’s race affect their extraversion?\nDoes extraversion decrease with age?\nDoes high agreeableness impact emotional stability?"
  }
]